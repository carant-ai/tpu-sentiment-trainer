{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "rating_mapping = {\n",
    "    1: 'negative',\n",
    "    2: 'negative',\n",
    "    3: 'unlabeled',\n",
    "    4: 'unlabeled',\n",
    "    5: 'positive'\n",
    "}\n",
    "rating_mapping = pd.DataFrame({\"index\":rating_mapping.keys(), \"label_text\":rating_mapping.values()}).set_index(\"index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_frame = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (hf) McAuley-Lab/Amazon-Reviews-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
    "frame = []\n",
    "\n",
    "for conf in ['raw_review_Grocery_and_Gourmet_Food', 'raw_review_Home_and_Kitchen']:\n",
    "    df = load_dataset(dataset_name, conf, split = 'full', trust_remote_code = True).to_pandas()\n",
    "    df = df.merge(rating_mapping, how = 'left', left_on = 'rating', right_on = 'index')\n",
    "    df = df[['text','label_text']]\n",
    "    df['source'] = dataset_name\n",
    "    df['split'] = conf\n",
    "    frame.append(df)\n",
    "\n",
    "dataset = pd.concat(frame)\n",
    "del frame, df\n",
    "english_frame.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (hf) imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "dataset_name = \"imdb\"\n",
    "frame = []\n",
    "\n",
    "label_mapping = {\n",
    "    0: 'negative',\n",
    "    -1: 'unlabeled',\n",
    "    1: 'positive'\n",
    "}\n",
    "label_mapping = pd.DataFrame({\"index\": label_mapping.keys(), \"label_text\": label_mapping.values()}).set_index(\"index\")\n",
    "\n",
    "for split in ['train', 'test', 'unsupervised']:\n",
    "    df = load_dataset(dataset_name, split = split, trust_remote_code = True).to_pandas()\n",
    "    df = df.merge(label_mapping, how = 'left', left_on = 'label', right_on = 'index')\n",
    "    df = df[['text','label_text']]\n",
    "    df['text'] = df['text'].apply(remove_html_tags)\n",
    "    df['source'] = dataset_name\n",
    "    df['split'] = split\n",
    "    frame.append(df)\n",
    "\n",
    "dataset = pd.concat(frame)\n",
    "del frame, df\n",
    "english_frame.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (hf) mteb/tweet_sentiment_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_link(text):\n",
    "    return re.sub(r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)', '', text).replace(\"&amp;\",\"&\")\n",
    "\n",
    "dataset_name = \"mteb/tweet_sentiment_extraction\"\n",
    "frame = []\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    df = load_dataset(dataset_name, split = split, trust_remote_code = True).to_pandas()\n",
    "    df = df[['text','label_text']]\n",
    "    df['text'] = df['text'].apply(remove_link)\n",
    "    df['source'] = dataset_name\n",
    "    df['split'] = split\n",
    "    frame.append(df)\n",
    "\n",
    "dataset = pd.concat(frame)\n",
    "del frame, df\n",
    "english_frame.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (kaggle) snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\"kaggle datasets download -d snap/amazon-fine-food-reviews\")\n",
    "subprocess.run(\"unzip amazon-fine-food-reviews.zip\")\n",
    "subprocess.run(\"rm amazon-fine-food-reviews.zip hashes.txt database.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Reviews.csv\")\n",
    "df = df.merge(rating_mapping, how = 'left', left_on = 'Score', right_on = 'index').rename(columns = {\"Text\": 'text'})\n",
    "dataset = df[['text','label_text']]\n",
    "del df\n",
    "dataset['source'] = dataset_name\n",
    "dataset['split'] = conf\n",
    "english_frame.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df = pd.concat(english_frame, ignore_index = True).drop_duplicates(\"text\")\n",
    "from datasets import Dataset\n",
    "english_dataset = Dataset.from_pandas(english_df)\n",
    "english_dataset.push_to_hub(\"thonyyy/english_sentiment_dataset\", private = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indonesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "indonesian_frame = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (hf) indonlp/indonlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"indonlp/indonlu\"\n",
    "conf = 'smsa'\n",
    "frame = []\n",
    "\n",
    "label_mapping = {\n",
    "    0: 'positive',\n",
    "    1: 'neutral',\n",
    "    2: 'negative'\n",
    "}\n",
    "label_mapping = pd.DataFrame({\"index\": label_mapping.keys(), \"label_text\": label_mapping.values()}).set_index(\"index\")\n",
    "\n",
    "for split in ['train', 'test', 'validation']:\n",
    "    df = load_dataset(dataset_name, conf, split = split, trust_remote_code = True).to_pandas()\n",
    "    df = df.merge(label_mapping, how = 'left', left_on = 'label', right_on = 'index')\n",
    "    df = df[['text','label_text']]\n",
    "    df['source'] = dataset_name + '/' + conf\n",
    "    df['split'] = split\n",
    "    frame.append(df)\n",
    "\n",
    "dataset = pd.concat(frame)\n",
    "del frame, df\n",
    "indonesian_frame.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (drive) Female Daily Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\"gdown --id 1smg2JQfz9tUf02ixpXGhkYN3zAkPQNQ_\")\n",
    "subprocess.run(\"gdown --id 12PWEk7vPrm0csj97kNGGmHz1Pu4Axd6Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reviews(filename):\n",
    "    reviews_list = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            review = json.loads(line)\n",
    "            reviews_list.append(review)\n",
    "    return reviews_list\n",
    "\n",
    "label_mapping = {\n",
    "    'neg': 'negative',\n",
    "    'neu': 'neutral',\n",
    "    'pos': 'positive'\n",
    "}\n",
    "label_mapping = pd.DataFrame({\"index\": label_mapping.keys(), \"label_text\": label_mapping.values()}).set_index(\"index\")\n",
    "\n",
    "frame = []\n",
    "for split in ['train','test']:\n",
    "    reviews = parse_reviews(f\"all_dataset_{split}.json\")\n",
    "    df = pd.DataFrame(reviews)\n",
    "    df = df.merge(label_mapping, how = 'left', left_on = \"review_class\", right_on = \"index\").rename(columns = {'review_text':'text'})\n",
    "    df = df[['text','label_text']]\n",
    "    df['split'] = split\n",
    "    df['source'] = \"FDR Dataset\"\n",
    "    frame.append(df)\n",
    "\n",
    "dataset = pd.concat(frame)\n",
    "indonesian_frame.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"intanm/indonesian-financial-sentiment-analysis\"\n",
    "frame = []\n",
    "\n",
    "label_mapping = {\n",
    "    0: 'negative',\n",
    "    1: 'neutral',\n",
    "    2: 'positive'\n",
    "    }\n",
    "label_mapping = pd.DataFrame({\"index\": label_mapping.keys(), \"label_text\": label_mapping.values()}).set_index(\"index\")\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    df = load_dataset(dataset_name, split = split, trust_remote_code = True).to_pandas()\n",
    "    df = df.merge(label_mapping, how = 'left', left_on = 'label', right_on = 'index')\n",
    "    df = df[['text','label_text']]\n",
    "    df['source'] = dataset_name\n",
    "    df['split'] = split\n",
    "    frame.append(df)\n",
    "\n",
    "dataset = pd.concat(frame)\n",
    "del frame, df\n",
    "indonesian_frame.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (kaggle) deniyulian/sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\"kaggle datasets download -d deniyulian/sentiment-analysis\")\n",
    "subprocess.run(\"unzip sentiment-analysis.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    -1: 'negative',\n",
    "    0: 'neutral',\n",
    "    1: 'positive'\n",
    "    }\n",
    "label_mapping = pd.DataFrame({\"index\": label_mapping.keys(), \"label_text\": label_mapping.values()}).set_index(\"index\")\n",
    "\n",
    "\n",
    "with open(\"dataset-idsa-master(1)/dataset-idsa-master/Indonesian Sentiment Twitter Dataset Labeled.csv\", \"r\") as file:\n",
    "    count = 0\n",
    "    for line in file:\n",
    "        if count == 0:\n",
    "            texts = []\n",
    "            labels = []\n",
    "        else:\n",
    "            stream = line.split(\"\\t\")\n",
    "            labels.append(int(stream[0]))\n",
    "            texts.append(stream[1])\n",
    "        count += 1\n",
    "\n",
    "df = pd.DataFrame({'text': texts, \"label\": labels})\n",
    "df = df.merge(label_mapping, how = 'left', left_on = 'label', right_on = 'index')\n",
    "df = df[['text','label_text']]\n",
    "df['source'] = \"deniyulian/sentiment-analysis\"\n",
    "df['split'] = 'poorly formated'\n",
    "indonesian_frame.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_text</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Banyak akun kloning seolah2 pendukung #agussil...</td>\n",
       "      <td>negative</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_pilkada_DKI_2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#agussilvy bicara apa kasihan yaa...lap itu ai...</td>\n",
       "      <td>negative</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_pilkada_DKI_2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kalau aku sih gak nunggu hasil akhir QC tp lag...</td>\n",
       "      <td>negative</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_pilkada_DKI_2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kasian oh kasian dengan peluru 1milyar untuk t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_pilkada_DKI_2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...</td>\n",
       "      <td>negative</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_pilkada_DKI_2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>Kali saja bpk @aniesbaswedan @sandiuno lihat, ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_pilkada_DKI_2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>Kita harus dapat merangkul semua orang tanpa b...</td>\n",
       "      <td>positive</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_pilkada_DKI_2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>Ini jagoanku dibidang digital &lt;Smiling Face Wi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_pilkada_DKI_2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>#PesanBijak #OkeOce #GubernurGu3 ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_pilkada_DKI_2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Sandiaga: Bangun Rumah DP 0% Lebih Simpel Diba...</td>\n",
       "      <td>positive</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_pilkada_DKI_2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text label_text  \\\n",
       "0    Banyak akun kloning seolah2 pendukung #agussil...   negative   \n",
       "1    #agussilvy bicara apa kasihan yaa...lap itu ai...   negative   \n",
       "2    Kalau aku sih gak nunggu hasil akhir QC tp lag...   negative   \n",
       "3    Kasian oh kasian dengan peluru 1milyar untuk t...   negative   \n",
       "4    Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...   negative   \n",
       "..                                                 ...        ...   \n",
       "895  Kali saja bpk @aniesbaswedan @sandiuno lihat, ...   positive   \n",
       "896  Kita harus dapat merangkul semua orang tanpa b...   positive   \n",
       "897  Ini jagoanku dibidang digital <Smiling Face Wi...   positive   \n",
       "898               #PesanBijak #OkeOce #GubernurGu3 ...   positive   \n",
       "899  Sandiaga: Bangun Rumah DP 0% Lebih Simpel Diba...   positive   \n",
       "\n",
       "                            source                   split  \n",
       "0    deniyulian/sentiment-analysis  tweet_pilkada_DKI_2017  \n",
       "1    deniyulian/sentiment-analysis  tweet_pilkada_DKI_2017  \n",
       "2    deniyulian/sentiment-analysis  tweet_pilkada_DKI_2017  \n",
       "3    deniyulian/sentiment-analysis  tweet_pilkada_DKI_2017  \n",
       "4    deniyulian/sentiment-analysis  tweet_pilkada_DKI_2017  \n",
       "..                             ...                     ...  \n",
       "895  deniyulian/sentiment-analysis  tweet_pilkada_DKI_2017  \n",
       "896  deniyulian/sentiment-analysis  tweet_pilkada_DKI_2017  \n",
       "897  deniyulian/sentiment-analysis  tweet_pilkada_DKI_2017  \n",
       "898  deniyulian/sentiment-analysis  tweet_pilkada_DKI_2017  \n",
       "899  deniyulian/sentiment-analysis  tweet_pilkada_DKI_2017  \n",
       "\n",
       "[900 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Dataset-Sentimen-Analisis-Bahasa-Indonesia/Dataset-Sentimen-Analisis-Bahasa-Indonesia-master/dataset_tweet_sentiment_pilkada_DKI_2017.csv\")\n",
    "df = df.rename(columns = {\"Text Tweet\": 'text', \"Sentiment\": \"label_text\"})\n",
    "df = df[['text', 'label_text']]\n",
    "df['source'] = \"deniyulian/sentiment-analysis\"\n",
    "df['split'] = 'tweet_pilkada_DKI_2017'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset-Sentimen-Analisis-Bahasa-Indonesia/Dataset-Sentimen-Analisis-Bahasa-Indonesia-master/dataset_tweet_sentiment_opini_film.csv\")\n",
    "df = df.rename(columns = {\"Text Tweet\": 'text', \"Sentiment\": \"label_text\"})\n",
    "df = df[['text', 'label_text']]\n",
    "df['source'] = \"deniyulian/sentiment-analysis\"\n",
    "df['split'] = 'tweet_opini_film'\n",
    "indonesian_frame.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset-Sentimen-Analisis-Bahasa-Indonesia/Dataset-Sentimen-Analisis-Bahasa-Indonesia-master/dataset_tweet_sentiment_cellular_service_provider.csv\")\n",
    "df = df.rename(columns = {\"Text Tweet\": 'text', \"Sentiment\": \"label_text\"})\n",
    "df = df[['text', 'label_text']]\n",
    "df['source'] = \"deniyulian/sentiment-analysis\"\n",
    "df['split'] = 'tweet_cellular_service_provider'\n",
    "indonesian_frame.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_text</th>\n",
       "      <th>source</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Undang @N_ShaniJKT48 ke hitamputih, pemenang S...</td>\n",
       "      <td>positive</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selamat berbuka puasa Semoga amal ibadah hari ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ada nih di trans7 hitam putih, dia dpt penghar...</td>\n",
       "      <td>positive</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>selamat ya mas @adietaufan masuk hitamputih</td>\n",
       "      <td>positive</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Asiknya nonton Hitam Putih Trans7</td>\n",
       "      <td>positive</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>ini apa banget deh gw paling kesel klo orang2 ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Orang miskin semakin miskin klo sekolah melaku...</td>\n",
       "      <td>negative</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>ga boLeh emosi, cepat tua, nonton #matanajwame...</td>\n",
       "      <td>negative</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>dr penampilan saja kyk preman taunya bkin kisr...</td>\n",
       "      <td>negative</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>Jawab aja ga usah berbelit-belit. Muter2 ga je...</td>\n",
       "      <td>negative</td>\n",
       "      <td>deniyulian/sentiment-analysis</td>\n",
       "      <td>tweet_tv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text label_text  \\\n",
       "0    Undang @N_ShaniJKT48 ke hitamputih, pemenang S...   positive   \n",
       "1    Selamat berbuka puasa Semoga amal ibadah hari ...   positive   \n",
       "2    Ada nih di trans7 hitam putih, dia dpt penghar...   positive   \n",
       "3          selamat ya mas @adietaufan masuk hitamputih   positive   \n",
       "4                    Asiknya nonton Hitam Putih Trans7   positive   \n",
       "..                                                 ...        ...   \n",
       "395  ini apa banget deh gw paling kesel klo orang2 ...   negative   \n",
       "396  Orang miskin semakin miskin klo sekolah melaku...   negative   \n",
       "397  ga boLeh emosi, cepat tua, nonton #matanajwame...   negative   \n",
       "398  dr penampilan saja kyk preman taunya bkin kisr...   negative   \n",
       "399  Jawab aja ga usah berbelit-belit. Muter2 ga je...   negative   \n",
       "\n",
       "                            source     split  \n",
       "0    deniyulian/sentiment-analysis  tweet_tv  \n",
       "1    deniyulian/sentiment-analysis  tweet_tv  \n",
       "2    deniyulian/sentiment-analysis  tweet_tv  \n",
       "3    deniyulian/sentiment-analysis  tweet_tv  \n",
       "4    deniyulian/sentiment-analysis  tweet_tv  \n",
       "..                             ...       ...  \n",
       "395  deniyulian/sentiment-analysis  tweet_tv  \n",
       "396  deniyulian/sentiment-analysis  tweet_tv  \n",
       "397  deniyulian/sentiment-analysis  tweet_tv  \n",
       "398  deniyulian/sentiment-analysis  tweet_tv  \n",
       "399  deniyulian/sentiment-analysis  tweet_tv  \n",
       "\n",
       "[400 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Dataset-Sentimen-Analisis-Bahasa-Indonesia/Dataset-Sentimen-Analisis-Bahasa-Indonesia-master/dataset_tweet_sentimen_tayangan_tv.csv\")\n",
    "df = df.rename(columns = {\"Text Tweet\": 'text', \"Sentiment\": \"label_text\"})\n",
    "df = df[['text', 'label_text']]\n",
    "df['source'] = \"deniyulian/sentiment-analysis\"\n",
    "df['split'] = 'tweet_tevision'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset-Sentimen-Analisis-Bahasa-Indonesia/Dataset-Sentimen-Analisis-Bahasa-Indonesia-master/dataset_komentar_instagram_cyberbullying.csv\")\n",
    "df = df.rename(columns = {\"Instagram Comment Text\": 'text', \"Sentiment\": \"label_text\"})\n",
    "df = df[['text', 'label_text']]\n",
    "df['source'] = \"deniyulian/sentiment-analysis\"\n",
    "df['split'] = 'instagram_comment_cyberbullying'\n",
    "indonesian_frame.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\"kaggle datasets download -d alexmariosimanjuntak/dana-app-sentiment-review-on-playstore-indonesia\")\n",
    "subprocess.run(\"unzip dana-app-sentiment-review-on-playstore-indonesia.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'POSITIVE': \"positive\",\n",
    "    'NEGATIVE': \"negative\",\n",
    "    'NEUTRAL': \"neutral\"\n",
    "    }\n",
    "\n",
    "label_mapping = pd.DataFrame({\"index\": label_mapping.keys(), \"label_text\": label_mapping.values()}).set_index(\"index\")\n",
    "\n",
    "df = pd.read_csv(\"review_dana_labelled.csv\")\n",
    "df = df.merge(label_mapping, how = 'left', left_on = 'sentimen', right_on = 'index').rename(columns = {\"content\" : \"text\"})\n",
    "df = df[['text','label_text']].drop_duplicates()\n",
    "df['source'] = \"alexmariosimanjuntak/dana-app-sentiment-review-on-playstore-indonesia\"\n",
    "df['split'] = \"train\"\n",
    "indonesian_frame.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\"kaggle datasets download -d bondanvitto/indonesia-twitter-comment-labeled-with-ite-law\")\n",
    "subprocess.run(\"unzip indonesia-twitter-comment-labeled-with-ite-law.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: \"neutral\",\n",
    "    1: \"positive\",\n",
    "    2: \"negative\",\n",
    "    3: \"negative\",\n",
    "    4: \"negative\",\n",
    "    5: \"negative\",\n",
    "    6: \"negative\",\n",
    "    }\n",
    "\n",
    "label_mapping = pd.DataFrame({\"index\": label_mapping.keys(), \"label_text\": label_mapping.values()}).set_index(\"index\")\n",
    "\n",
    "df = pd.read_csv(\"Dataset Twitter Fix - Indonesian Sentiment Twitter Dataset Labeled (1).csv\")\n",
    "df = df.merge(label_mapping, how = 'left', left_on = 'sentimen', right_on = 'index').rename(columns = {\"Tweet\" : \"text\"})\n",
    "df = df[['text','label_text']].drop_duplicates()\n",
    "df['source'] = \"bondanvitto/indonesia-twitter-comment-labeled-with-ite-law\"\n",
    "df['split'] = \"train\"\n",
    "indonesian_frame.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\"kaggle datasets download -d christofel04/review-lapak-sentiment\")\n",
    "subprocess.run(\"unzip review-lapak-sentiment.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: \"negative\",\n",
    "    1: \"positive\"\n",
    "}\n",
    "label_mapping = pd.DataFrame({\"index\": label_mapping.keys(), \"label_text\": label_mapping.values()}).set_index(\"index\")\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df = df.merge(label_mapping, how = 'left', left_on = 'label', right_on = 'index').rename(columns = {\"review_sangat_singkat\" : \"text\"})\n",
    "df = df[['text','label_text']].drop_duplicates()\n",
    "df['source'] = \"christofel04/review-lapak-sentiment\"\n",
    "df['split'] = \"train\"\n",
    "indonesian_frame.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\"kaggle datasets download -d taqiyyaghazi/indonesian-marketplace-product-reviews\")\n",
    "subprocess.run(\"unzip indonesian-marketplace-product-reviews.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: \"negative\",\n",
    "    1: \"positive\"\n",
    "}\n",
    "label_mapping = pd.DataFrame({\"index\": label_mapping.keys(), \"label_text\": label_mapping.values()}).set_index(\"index\")\n",
    "\n",
    "df = pd.read_csv(\"reviews.csv\")\n",
    "df = df.merge(label_mapping, how = 'left', left_on = 'label', right_on = 'index').rename(columns = {\"reviews\" : \"text\"})\n",
    "df = df[['text','label_text']].drop_duplicates()\n",
    "df['source'] = \"taqiyyaghazi/indonesian-marketplace-product-reviews\"\n",
    "df['split'] = \"train\"\n",
    "indonesian_frame.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\"kaggle datasets download -d yudhaislamisulistya/jokowi-tweets\")\n",
    "subprocess.run(\"unzip jokowi-tweets.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'positif': \"positive\",\n",
    "    'negatif': \"negative\",\n",
    "    'netral': \"neutral\"\n",
    "    }\n",
    "label_mapping = pd.DataFrame({\"index\": label_mapping.keys(), \"label_text\": label_mapping.values()}).set_index(\"index\")\n",
    "\n",
    "df = pd.read_csv(\"Tweet Bapak Jokowi - Tweet Bapak Jokowi.csv\")\n",
    "df = df.merge(label_mapping, how = 'left', left_on = 'Label', right_on = 'index')\n",
    "df = df[['text','label_text']].drop_duplicates()\n",
    "df['source'] = \"yudhaislamisulistya/jokowi-tweets\"\n",
    "df['split'] = \"train\"\n",
    "indonesian_frame.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\"kaggle datasets download -d itanium/livin-by-mandiri-app-reviews\")\n",
    "subprocess.run(\"unzip livin-by-mandiri-app-reviews.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mandiri.csv\")\n",
    "df = df.merge(rating_mapping, how = 'left', left_on = 'rating', right_on = 'index').rename(columns = {\"review\" : \"text\"})\n",
    "df = df[['text','label_text']].drop_duplicates()\n",
    "df['source'] = \"itanium/livin-by-mandiri-app-reviews\"\n",
    "df['split'] = \"train\"\n",
    "indonesian_frame.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\"kaggle datasets download -d jocelyndumlao/prdect-id-indonesian-emotion-classification\")\n",
    "subprocess.run(\"unzip prdect-id-indonesian-emotion-classification.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    \"Negative\": \"negative\",\n",
    "    \"Positive\": \"positive\"\n",
    "}\n",
    "label_mapping = pd.DataFrame({\"index\": label_mapping.keys(), \"label_text\": label_mapping.values()}).set_index(\"index\")\n",
    "\n",
    "df = pd.read_csv(\"Product Reviews Dataset for Emotions Classification Tasks - Indonesian (PRDECT-ID) Dataset/PRDECT-ID Dataset.csv\")\n",
    "df = df.merge(label_mapping, how = 'left', left_on = 'Sentiment', right_on = 'index').rename(columns = {\"Customer Review\" : \"text\"})\n",
    "df = df[['text','label_text']].drop_duplicates()\n",
    "df['source'] = \"jocelyndumlao/prdect-id-indonesian-emotion-classification\"\n",
    "df['split'] = \"train\"\n",
    "indonesian_frame.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\"kaggle datasets download -d anggapurnama/twitter-dataset-ppkm\")\n",
    "subprocess.run(\"unzip twitter-dataset-ppkm.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: \"positive\",\n",
    "    1: \"neutral\",\n",
    "    2: \"negative\",\n",
    "    }\n",
    "label_mapping = pd.DataFrame({\"index\": label_mapping.keys(), \"label_text\": label_mapping.values()}).set_index(\"index\")\n",
    "\n",
    "df = pd.read_csv(\"INA_TweetsPPKM_Labeled_Pure.csv\", delimiter = \"\\t\")\n",
    "df = df.merge(label_mapping, how = 'left', left_on = 'sentiment', right_on = 'index').rename(columns = {\"Tweet\" : \"text\"})\n",
    "df = df[['text','label_text']].drop_duplicates()\n",
    "df['source'] = \"yudhaislamisulistya/jokowi-tweets\"\n",
    "df['split'] = \"train\"\n",
    "indonesian_frame.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\"kaggle datasets download -d grikomsn/lazada-indonesian-reviews\")\n",
    "subprocess.run(\"unzip lazada-indonesian-reviews.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"20191002-reviews.csv\")\n",
    "df = df.merge(rating_mapping, how = 'left', left_on = 'rating', right_on = 'index').rename(columns = {\"reviewContent\" : \"text\"})\n",
    "df = df[['text','label_text']].drop_duplicates().dropna()\n",
    "df['source'] = \"grikomsn/lazada-indonesian-reviews\"\n",
    "df['split'] = \"train\"\n",
    "indonesian_frame.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1031/1031 [00:02<00:00, 510.75ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:07<00:00,  7.75s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/thonyyy/indonesian_sentiment_dataset/commit/d364f7c56811d7fdfb7adab84f3e8f66265a8152', commit_message='Upload dataset', commit_description='', oid='d364f7c56811d7fdfb7adab84f3e8f66265a8152', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indonesian_df = pd.concat(indonesian_frame, ignore_index = True).drop_duplicates(\"text\")\n",
    "from datasets import Dataset\n",
    "indonesian_dataset = Dataset.from_pandas(indonesian_df)\n",
    "indonesian_dataset.push_to_hub(\"thonyyy/indonesian_sentiment_dataset\", private = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
